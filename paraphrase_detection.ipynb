{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Paraphrase Classification Model\r\n",
    "\r\n",
    "## Data Loading\r\n",
    "\r\n",
    "Load the training and validation data into dataframes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "train_df = pd.read_csv('./data/paws/train.tsv', sep='\\t')[['sentence1', 'sentence2', 'label']]\r\n",
    "val_df = pd.read_csv('./data/paws/dev.tsv', sep='\\t')[['sentence1', 'sentence2', 'label']]\r\n",
    "train_df.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  In Paris , in October 1560 , he secretly met t...   \n",
       "1  The NBA season of 1975 -- 76 was the 30th seas...   \n",
       "2  There are also specific discussions , public p...   \n",
       "3  When comparable rates of flow can be maintaine...   \n",
       "4  It is the seat of Zerendi District in Akmola R...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  In October 1560 , he secretly met with the Eng...      0  \n",
       "1  The 1975 -- 76 season of the National Basketba...      1  \n",
       "2  There are also public discussions , profile sp...      0  \n",
       "3  The results are high when comparable flow rate...      1  \n",
       "4  It is the seat of the district of Zerendi in A...      1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Paris , in October 1560 , he secretly met t...</td>\n",
       "      <td>In October 1560 , he secretly met with the Eng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The NBA season of 1975 -- 76 was the 30th seas...</td>\n",
       "      <td>The 1975 -- 76 season of the National Basketba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are also specific discussions , public p...</td>\n",
       "      <td>There are also public discussions , profile sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When comparable rates of flow can be maintaine...</td>\n",
       "      <td>The results are high when comparable flow rate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is the seat of Zerendi District in Akmola R...</td>\n",
       "      <td>It is the seat of the district of Zerendi in A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the dataframes to create huggingface datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from datasets import Dataset\r\n",
    "\r\n",
    "train_dataset = Dataset.from_pandas(train_df)\r\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize Data\r\n",
    "\r\n",
    "Load the tokenizer of the model that will be fine-tuned"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\r\n",
    "\r\n",
    "checkpoint = 'sentence-transformers/all-distilroberta-v1'\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tokenize function tokenizes both sentences and concatenates them with a seperator token"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def tokenize_function(examples):\r\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the tokenize function to each dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\r\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\r\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  8.85ba/s]\n",
      "100%|██████████| 8/8 [00:00<00:00,  8.04ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model\r\n",
    "\r\n",
    "Load the huggingface model for sequence classification with 2 labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from transformers import AutoModelForSequenceClassification\r\n",
    "\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to('cuda')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/all-distilroberta-v1 were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-distilroberta-v1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Trainer Config\r\n",
    "\r\n",
    "Load the accuracy and f1 metrics and create a function that applies them to pass to the trainer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\r\n",
    "from datasets import load_metric\r\n",
    "\r\n",
    "metric = load_metric('accuracy', 'f1')\r\n",
    "\r\n",
    "def compute_metrics(eval_preds):\r\n",
    "    logits, labels = eval_preds\r\n",
    "    predictions = np.argmax(logits, axis=-1)\r\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the TrainingArguments. This specifies checkpoint path, batch_size, epochs, when to apply the metrics, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from transformers import TrainingArguments\r\n",
    "batch_size = 64\r\n",
    "training_args = TrainingArguments('./data/models/test_paraphrase_distilbert_1', per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, num_train_epochs=5, evaluation_strategy='epoch', report_to=\"wandb\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\r\n",
    "Create the Trainer with everything created so far and run the train function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from transformers import Trainer\r\n",
    "\r\n",
    "trainer = Trainer(\r\n",
    "    model,\r\n",
    "    training_args,\r\n",
    "    train_dataset=tokenized_train,\r\n",
    "    eval_dataset=tokenized_val,\r\n",
    "    data_collator=data_collator,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    compute_metrics=compute_metrics\r\n",
    ")\r\n",
    "train_output = trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running training *****\n",
      "  Num examples = 49401\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3860\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphillipmerritt\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "c:\\VSCodeDrive\\5290_text_summarization\\env\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/phillipmerritt/huggingface/runs/30shsa0p\" target=\"_blank\">./data/models/test_paraphrase_distilbert_1</a></strong> to <a href=\"https://wandb.ai/phillipmerritt/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 13%|█▎        | 500/3860 [02:24<21:34,  2.60it/s]Saving model checkpoint to ./data/models/test_paraphrase_distilbert_1\\checkpoint-500\n",
      "Configuration saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.6317, 'learning_rate': 4.352331606217617e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-500\\special_tokens_map.json\n",
      " 20%|██        | 772/3860 [03:39<13:23,  3.84it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 20%|██        | 772/3860 [03:50<13:23,  3.84it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.3568389415740967, 'eval_accuracy': 0.8465, 'eval_runtime': 11.853, 'eval_samples_per_second': 674.934, 'eval_steps_per_second': 10.546, 'epoch': 1.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 26%|██▌       | 1000/3860 [04:52<17:38,  2.70it/s]Saving model checkpoint to ./data/models/test_paraphrase_distilbert_1\\checkpoint-1000\n",
      "Configuration saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.3537, 'learning_rate': 3.704663212435233e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1000\\special_tokens_map.json\n",
      " 39%|███▉      | 1500/3860 [07:07<16:05,  2.44it/s]Saving model checkpoint to ./data/models/test_paraphrase_distilbert_1\\checkpoint-1500\n",
      "Configuration saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.2486, 'learning_rate': 3.05699481865285e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-1500\\special_tokens_map.json\n",
      " 40%|████      | 1544/3860 [07:21<09:41,  3.98it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 40%|████      | 1544/3860 [07:33<09:41,  3.98it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.2868412435054779, 'eval_accuracy': 0.8935, 'eval_runtime': 11.6285, 'eval_samples_per_second': 687.967, 'eval_steps_per_second': 10.749, 'epoch': 2.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 52%|█████▏    | 2000/3860 [09:33<12:32,  2.47it/s]Saving model checkpoint to ./data/models/test_paraphrase_distilbert_1\\checkpoint-2000\n",
      "Configuration saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.1715, 'learning_rate': 2.4093264248704665e-05, 'epoch': 2.59}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2000\\special_tokens_map.json\n",
      " 60%|██████    | 2316/3860 [10:59<06:41,  3.84it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 60%|██████    | 2316/3860 [11:11<06:41,  3.84it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.3055209815502167, 'eval_accuracy': 0.897375, 'eval_runtime': 11.7392, 'eval_samples_per_second': 681.477, 'eval_steps_per_second': 10.648, 'epoch': 3.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 65%|██████▍   | 2500/3860 [12:00<08:49,  2.57it/s]Saving model checkpoint to ./data/models/test_paraphrase_distilbert_1\\checkpoint-2500\n",
      "Configuration saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.1411, 'learning_rate': 1.761658031088083e-05, 'epoch': 3.24}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-2500\\special_tokens_map.json\n",
      " 78%|███████▊  | 3000/3860 [14:16<05:48,  2.47it/s]Saving model checkpoint to ./data/models/test_paraphrase_distilbert_1\\checkpoint-3000\n",
      "Configuration saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.1139, 'learning_rate': 1.1139896373056995e-05, 'epoch': 3.89}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3000\\special_tokens_map.json\n",
      " 80%|████████  | 3088/3860 [14:42<03:21,  3.82it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 80%|████████  | 3088/3860 [14:53<03:21,  3.82it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.3860807418823242, 'eval_accuracy': 0.904875, 'eval_runtime': 11.8483, 'eval_samples_per_second': 675.204, 'eval_steps_per_second': 10.55, 'epoch': 4.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 91%|█████████ | 3500/3860 [16:44<02:20,  2.56it/s]Saving model checkpoint to ./data/models/test_paraphrase_distilbert_1\\checkpoint-3500\n",
      "Configuration saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.0863, 'learning_rate': 4.663212435233161e-06, 'epoch': 4.53}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/test_paraphrase_distilbert_1\\checkpoint-3500\\special_tokens_map.json\n",
      "100%|██████████| 3860/3860 [18:23<00:00,  3.84it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 3860/3860 [18:35<00:00,  3.84it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 3860/3860 [18:35<00:00,  3.46it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.38545864820480347, 'eval_accuracy': 0.90675, 'eval_runtime': 11.8056, 'eval_samples_per_second': 677.642, 'eval_steps_per_second': 10.588, 'epoch': 5.0}\n",
      "{'train_runtime': 1123.8149, 'train_samples_per_second': 219.792, 'train_steps_per_second': 3.435, 'train_loss': 0.23392994935030764, 'epoch': 5.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_output"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3860, training_loss=0.22098883544842815, metrics={'train_runtime': 1111.6416, 'train_samples_per_second': 222.198, 'train_steps_per_second': 3.472, 'train_loss': 0.22098883544842815, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('env')"
  },
  "interpreter": {
   "hash": "298c5e6131db63fa5faf60684f80d59bba7e2193feaf38ca4b23014a2148fcef"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}