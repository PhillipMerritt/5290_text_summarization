{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Paraphrase Classification Model\r\n",
    "\r\n",
    "## Data Loading\r\n",
    "\r\n",
    "Load the training and validation data into dataframes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "train_df = pd.read_csv('./data/paws/train.tsv', sep='\\t')[['sentence1', 'sentence2', 'label']]\r\n",
    "val_df = pd.read_csv('./data/paws/dev.tsv', sep='\\t')[['sentence1', 'sentence2', 'label']]\r\n",
    "train_df.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  In Paris , in October 1560 , he secretly met t...   \n",
       "1  The NBA season of 1975 -- 76 was the 30th seas...   \n",
       "2  There are also specific discussions , public p...   \n",
       "3  When comparable rates of flow can be maintaine...   \n",
       "4  It is the seat of Zerendi District in Akmola R...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  In October 1560 , he secretly met with the Eng...      0  \n",
       "1  The 1975 -- 76 season of the National Basketba...      1  \n",
       "2  There are also public discussions , profile sp...      0  \n",
       "3  The results are high when comparable flow rate...      1  \n",
       "4  It is the seat of the district of Zerendi in A...      1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Paris , in October 1560 , he secretly met t...</td>\n",
       "      <td>In October 1560 , he secretly met with the Eng...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The NBA season of 1975 -- 76 was the 30th seas...</td>\n",
       "      <td>The 1975 -- 76 season of the National Basketba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are also specific discussions , public p...</td>\n",
       "      <td>There are also public discussions , profile sp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When comparable rates of flow can be maintaine...</td>\n",
       "      <td>The results are high when comparable flow rate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is the seat of Zerendi District in Akmola R...</td>\n",
       "      <td>It is the seat of the district of Zerendi in A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the dataframes to create huggingface datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from datasets import Dataset\r\n",
    "\r\n",
    "train_dataset = Dataset.from_pandas(train_df)\r\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize Data\r\n",
    "\r\n",
    "Load the tokenizer of the model that will be fine-tuned"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\r\n",
    "\r\n",
    "checkpoint = 'sentence-transformers/all-distilroberta-v1'\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tokenize function tokenizes both sentences and concatenates them with a seperator token"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def tokenize_function(examples):\r\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the tokenize function to each dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\r\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\r\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.43ba/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 12.52ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model\r\n",
    "\r\n",
    "Load the huggingface model for sequence classification with 2 labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from transformers import AutoModelForSequenceClassification\r\n",
    "\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to('cuda')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/all-distilroberta-v1 were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-distilroberta-v1 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Trainer Config\r\n",
    "\r\n",
    "Load the accuracy and f1 metrics and create a function that applies them to pass to the trainer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np\r\n",
    "from datasets import load_metric\r\n",
    "\r\n",
    "metric = load_metric('accuracy', 'f1')\r\n",
    "\r\n",
    "def compute_metrics(eval_preds):\r\n",
    "    logits, labels = eval_preds\r\n",
    "    predictions = np.argmax(logits, axis=-1)\r\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the TrainingArguments. This specifies checkpoint path, batch_size, epochs, when to apply the metrics, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from transformers import TrainingArguments\r\n",
    "batch_size = 64\r\n",
    "training_args = TrainingArguments('./data/models/paraphrase_distilbert_1', per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, num_train_epochs=5, evaluation_strategy='epoch', report_to=\"wandb\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\r\n",
    "Create the Trainer with everything created so far and run the train function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from transformers import Trainer\r\n",
    "\r\n",
    "trainer = Trainer(\r\n",
    "    model,\r\n",
    "    training_args,\r\n",
    "    train_dataset=tokenized_train,\r\n",
    "    eval_dataset=tokenized_val,\r\n",
    "    data_collator=data_collator,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    compute_metrics=compute_metrics\r\n",
    ")\r\n",
    "train_output = trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running training *****\n",
      "  Num examples = 49401\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3860\n",
      " 13%|█▎        | 500/3860 [02:17<14:18,  3.91it/s]Saving model checkpoint to ./data/models/paraphrase_distilbert_1\\checkpoint-500\n",
      "Configuration saved in ./data/models/paraphrase_distilbert_1\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.5628, 'learning_rate': 4.352331606217617e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/paraphrase_distilbert_1\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-500\\special_tokens_map.json\n",
      " 20%|██        | 772/3860 [03:31<13:02,  3.95it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 20%|██        | 772/3860 [03:43<13:02,  3.95it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.34087324142456055, 'eval_accuracy': 0.8545, 'eval_runtime': 11.7322, 'eval_samples_per_second': 681.884, 'eval_steps_per_second': 10.654, 'epoch': 1.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 26%|██▌       | 1000/3860 [04:43<11:47,  4.04it/s]Saving model checkpoint to ./data/models/paraphrase_distilbert_1\\checkpoint-1000\n",
      "Configuration saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.3139, 'learning_rate': 3.704663212435233e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1000\\special_tokens_map.json\n",
      " 39%|███▉      | 1500/3860 [06:57<10:26,  3.76it/s]Saving model checkpoint to ./data/models/paraphrase_distilbert_1\\checkpoint-1500\n",
      "Configuration saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.2326, 'learning_rate': 3.05699481865285e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-1500\\special_tokens_map.json\n",
      " 40%|████      | 1544/3860 [07:10<09:27,  4.08it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 40%|████      | 1544/3860 [07:22<09:27,  4.08it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.2979626953601837, 'eval_accuracy': 0.892875, 'eval_runtime': 11.6615, 'eval_samples_per_second': 686.015, 'eval_steps_per_second': 10.719, 'epoch': 2.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 52%|█████▏    | 2000/3860 [09:24<08:46,  3.53it/s]Saving model checkpoint to ./data/models/paraphrase_distilbert_1\\checkpoint-2000\n",
      "Configuration saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.1639, 'learning_rate': 2.4093264248704665e-05, 'epoch': 2.59}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2000\\special_tokens_map.json\n",
      " 60%|██████    | 2316/3860 [10:50<06:24,  4.02it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 60%|██████    | 2316/3860 [11:01<06:24,  4.02it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.3627687394618988, 'eval_accuracy': 0.8925, 'eval_runtime': 11.5903, 'eval_samples_per_second': 690.235, 'eval_steps_per_second': 10.785, 'epoch': 3.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 65%|██████▍   | 2500/3860 [11:50<05:53,  3.84it/s]Saving model checkpoint to ./data/models/paraphrase_distilbert_1\\checkpoint-2500\n",
      "Configuration saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.1344, 'learning_rate': 1.761658031088083e-05, 'epoch': 3.24}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-2500\\special_tokens_map.json\n",
      " 78%|███████▊  | 3000/3860 [14:04<03:49,  3.74it/s]Saving model checkpoint to ./data/models/paraphrase_distilbert_1\\checkpoint-3000\n",
      "Configuration saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.1079, 'learning_rate': 1.1139896373056995e-05, 'epoch': 3.89}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3000\\special_tokens_map.json\n",
      " 80%|████████  | 3088/3860 [14:29<03:19,  3.87it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      " 80%|████████  | 3088/3860 [14:40<03:19,  3.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.4107610583305359, 'eval_accuracy': 0.903375, 'eval_runtime': 11.5786, 'eval_samples_per_second': 690.928, 'eval_steps_per_second': 10.796, 'epoch': 4.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 91%|█████████ | 3500/3860 [16:28<01:33,  3.84it/s]Saving model checkpoint to ./data/models/paraphrase_distilbert_1\\checkpoint-3500\n",
      "Configuration saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 0.0821, 'learning_rate': 4.663212435233161e-06, 'epoch': 4.53}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Model weights saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./data/models/paraphrase_distilbert_1\\checkpoint-3500\\special_tokens_map.json\n",
      "100%|██████████| 3860/3860 [18:04<00:00,  3.97it/s]The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8000\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 3860/3860 [18:15<00:00,  3.97it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 3860/3860 [18:15<00:00,  3.52it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.42276668548583984, 'eval_accuracy': 0.90225, 'eval_runtime': 11.5793, 'eval_samples_per_second': 690.89, 'eval_steps_per_second': 10.795, 'epoch': 5.0}\n",
      "{'train_runtime': 1095.7058, 'train_samples_per_second': 225.43, 'train_steps_per_second': 3.523, 'train_loss': 0.21426511626169473, 'epoch': 5.0}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "train_output"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3860, training_loss=0.21426511626169473, metrics={'train_runtime': 1095.7058, 'train_samples_per_second': 225.43, 'train_steps_per_second': 3.523, 'train_loss': 0.21426511626169473, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('env')"
  },
  "interpreter": {
   "hash": "298c5e6131db63fa5faf60684f80d59bba7e2193feaf38ca4b23014a2148fcef"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}